import os
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import argparse
from param import parse_args
from hum import HUM

class EmptyMLP(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, x):
        # Return something that, when added to residual, does nothing.
        # In Qwen2: hidden_states = residual + hidden_states_from_mlp
        # So we return 0. (Broadcasting will handle the shape or we can use zeros_like)
        return 0

def prune_mlp_layers(model, layers_to_prune):
    """
    Replaces the MLP sub-layer with EmptyMLP for specified layer indices.
    """
    print("DEBUG: Searching for layers in model structure...")
    m = model.llm
    layers = None
    
    # Try common Qwen2 structures
    curr = m
    for i in range(10): # Prevent infinite loop
        print(f"DEBUG: Level {i}, Current type: {type(curr)}")
        if hasattr(curr, "layers"):
            layers = curr.layers
            print("DEBUG: Found layers directly.")
            break
        if hasattr(curr, "model") and hasattr(curr.model, "layers"):
            layers = curr.model.layers
            print("DEBUG: Found layers in .model.layers")
            break
        
        # Try unwrapping PEFT or CausalLM
        if hasattr(curr, "base_model") and curr.base_model is not curr:
            curr = curr.base_model
        elif hasattr(curr, "model") and curr.model is not curr:
            curr = curr.model
        else:
            break

    if layers is None:
        print("DEBUG: Generic search failed, checking common attributes...")
        # Fallback to direct attribute check
        if hasattr(m, "layers"): layers = m.layers
        elif hasattr(m, "model") and hasattr(m.model, "layers"): layers = m.model.layers
        elif hasattr(m, "base_model") and hasattr(m.base_model, "model") and hasattr(m.base_model.model, "model") and hasattr(m.base_model.model.model, "layers"):
            layers = m.base_model.model.model.layers
        
    if layers is None:
        raise AttributeError("Could not find 'layers' in model. Check architecture.")

    for idx in layers_to_prune:
        print(f"Pruning MLP in layer {idx}...")
        layers[idx].mlp = EmptyMLP()
    
    return model

def main():
    parser = argparse.ArgumentParser(description="Prune redundant MLP layers from Qwen2")
    parser.add_argument("--config", type=str, required=True, help="Path to HUM config yaml")
    parser.add_argument("--scores_path", type=str, required=True, help="Path to the JSON scores file generated by calculate_importance.py")
    parser.add_argument("--top_k", type=int, default=8, help="Number of MLP layers to prune (lowest scoring)")
    parser.add_argument("--save_path", type=str, default="ckp/pruned_model.pth", help="Path to save the pruned model weights")
    parser.add_argument("--load_path", type=str, help="Path to load existing model weights (optional)")
    
    args_cli = parser.parse_args()
    
    # Load HUM args for model initialization
    import sys
    # Mock sys.argv to use our config with HUM's parse_args
    sys.argv = [sys.argv[0], "--config", args_cli.config]
    from param import parse_args
    args_hum = parse_args()
    args_hum.rank = 0
    args_hum.gpu = 0
    args_hum.num_gpus = 1

    # Load Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args_hum.root_path + args_hum.backbone, padding_side="left")
    special_tokens_dict = {'additional_special_tokens': ['<|emb|>', '<|thought|>']}
    tokenizer.add_special_tokens(special_tokens_dict)

    # Initialize Model
    model = HUM(args_hum, tokenizer)
    if args_cli.load_path:
        print(f"Loading weights from {args_cli.load_path}")
        weights = torch.load(args_cli.load_path, map_location='cpu')
        model.load_state_dict(weights, strict=False)

    # Load Scores
    with open(args_cli.scores_path, "r") as f:
        scores = json.load(f)
    
    mlp_scores = np.array(scores["mlp"])
    # Get indices of the k smallest scores
    prune_indices = np.argsort(mlp_scores)[:args_cli.top_k].tolist()
    print(f"Identified MLP layers to prune: {prune_indices}")

    # Prune
    model = prune_mlp_layers(model, prune_indices)

    # Save
    os.makedirs(os.path.dirname(args_cli.save_path), exist_ok=True)
    # Only save trainable parameters OR the whole thing? 
    # Usually for inference we save the whole state dict or just the diff.
    # Since we changed architecture, we should save the state dict.
    # But wait, torch.save(model.state_dict()) might have issues loading if the class structure is different.
    # However, our HUM class is the same, and we modified the instance.
    
    # Let's save the full state dict.
    print(f"Saving pruned model to {args_cli.save_path} ... (this might take a minute)")
    torch.save(model.state_dict(), args_cli.save_path)
    print(f"Pruned model saved to {args_cli.save_path}")

    # Save pruning info
    with open(args_cli.save_path + ".info.json", "w") as f:
        json.dump({"pruned_mlp_layers": prune_indices}, f)

if __name__ == "__main__":
    import numpy as np
    main()
