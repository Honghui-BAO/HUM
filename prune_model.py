import os
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import argparse
from param import parse_args
from hum import HUM

class EmptyMLP(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, x):
        # Return something that, when added to residual, does nothing.
        # In Qwen2: hidden_states = residual + hidden_states_from_mlp
        # So we return 0. (Broadcasting will handle the shape or we can use zeros_like)
        return 0

def prune_mlp_layers(model, layers_to_prune):
    """
    Replaces the MLP sub-layer with EmptyMLP for specified layer indices.
    """
    llm = model.llm
    if hasattr(llm, "base_model"):
        layers = llm.base_model.model.model.layers
    else:
        layers = llm.model.layers

    for idx in layers_to_prune:
        print(f"Pruning MLP in layer {idx}...")
        layers[idx].mlp = EmptyMLP()
    
    return model

def main():
    parser = argparse.ArgumentParser(description="Prune redundant MLP layers from Qwen2")
    parser.add_argument("--config", type=str, required=True, help="Path to HUM config yaml")
    parser.add_argument("--scores_path", type=str, required=True, help="Path to the JSON scores file generated by calculate_importance.py")
    parser.add_argument("--top_k", type=int, default=8, help="Number of MLP layers to prune (lowest scoring)")
    parser.add_argument("--save_path", type=str, default="ckp/pruned_model.pth", help="Path to save the pruned model weights")
    parser.add_argument("--load_path", type=str, help="Path to load existing model weights (optional)")
    
    args_cli = parser.parse_args()
    
    # Load HUM args for model initialization
    import sys
    # Mock sys.argv to use our config with HUM's parse_args
    sys.argv = [sys.argv[0], "--config", args_cli.config]
    from param import parse_args
    args_hum = parse_args()

    # Load Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args_hum.root_path + args_hum.backbone, padding_side="left")
    special_tokens_dict = {'additional_special_tokens': ['<|emb|>', '<|thought|>']}
    tokenizer.add_special_tokens(special_tokens_dict)

    # Initialize Model
    model = HUM(args_hum, tokenizer)
    if args_cli.load_path:
        print(f"Loading weights from {args_cli.load_path}")
        weights = torch.load(args_cli.load_path, map_location='cpu')
        model.load_state_dict(weights, strict=False)

    # Load Scores
    with open(args_cli.scores_path, "r") as f:
        scores = json.load(f)
    
    mlp_scores = np.array(scores["mlp"])
    # Get indices of the k smallest scores
    prune_indices = np.argsort(mlp_scores)[:args_cli.top_k].tolist()
    print(f"Identified MLP layers to prune: {prune_indices}")

    # Prune
    model = prune_mlp_layers(model, prune_indices)

    # Save
    os.makedirs(os.path.dirname(args_cli.save_path), exist_ok=True)
    # Only save trainable parameters OR the whole thing? 
    # Usually for inference we save the whole state dict or just the diff.
    # Since we changed architecture, we should save the state dict.
    # But wait, torch.save(model.state_dict()) might have issues loading if the class structure is different.
    # However, our HUM class is the same, and we modified the instance.
    
    # Let's save the full state dict.
    torch.save(model.state_dict(), args_cli.save_path)
    print(f"Pruned model saved to {args_cli.save_path}")

    # Save pruning info
    with open(args_cli.save_path + ".info.json", "w") as f:
        json.dump({"pruned_mlp_layers": prune_indices}, f)

if __name__ == "__main__":
    import numpy as np
    main()
